#!/bin/bash

#This script takes one or more URLs as arguments
#It loops through these URLs grabbing the HTML source,
#    parsing this source for img tags,
#    grabbing the source URL
#    and then downloading any images from these URLs.
#These image files are placed in a direcetory with the name of the URL
#    minus http://www and any other . / or :
#Next all but the largest image is deleted
#    this image is renamed to cover.jpg

for var in "$@"
do
  f=`echo $var | \
    sed 's_http:__g' | sed 's_//www.__g' | \
    sed 's_[/.:]__g'`

  echo $var | wget -q -O - -i -  | \
  sed 's_img_\n&_g' | grep "^img" | \
  sed 's_src[\s]*=[\s]*"_\n&_g' | \
  grep "^src" | \
  sed 's_src[\s]*=[\s]*"[\s]*__g' | \
  grep "^http" | \
  sed 's_"_\n"_g' | \
  grep "^http" > test
  cat test | egrep "jpg$|jpeg$" | \
  wget -i - -P ./$f -A jpg,jpeg -q -nc
  find ./$f -type f -print0 | xargs -0 du -s | \
  sort -rn | tail -n +2 | sed 's_\._\n\._' | \
  grep '^\.' | sed 's_ _\\ _g' > test
  cat test | xargs rm -f
  if ls -U ./$f/*.jpg > /dev/null 2>&1
  then
    mv ./$f/*.jpg ./$f/cover.jpg
  fi
  if ls -U ./$f/*.jpeg > /dev/null 2>&1
  then
    mv ./$f/*.jpeg ./$f/cover.jpg
  fi
done

